\section{Теоретическая часть} \label{sec:theory}
Для начала нам нужно было очистить данные от статистических выбросов -- таким образом, мы избавились от неоднозначных ответов, на подобии тематики секции, названий онлайн-сервисов, которые использовали ученики. 
Так же мы убрали из данных противоречивые ответы на вопросы вопросы, ответы, в которых почти на все вопросы был дан ответ с одинаковой нумерацией (например, школьник на все вопросы отвечал \enquote{Да} или \enquote{1}). 
Над оставшимся массивом данных уже можно было проводить преобразования для отображения ответов каждого респондента в пространство $\{0, 1\}^{m}$. 
Для этого мы воспользовались тремя разными алгоритмами: {\bf Ordinal Encoding}, {\bf One-Hot Encoding} и {\bf Dummy Variable Encoding}.
\subsection{Ordinal Encoding}

В случае алгоритма {\bf Ordinal Encoding}, каждой уникальной категории присваивается уникальное целое число.
Например, категории \enquote{Да}, \enquote{Нет} и \enquote{Затрудняюсь ответить} могут быть закодированы через последовательность 1, 2, 3.
Поскольку такое кодирование является весьма естественным и легко обратимым, мы попробовали применить этот алгоритм для кодирования наших данных, однако быстро отказались от этой идеи.
Поскольку такое кодирование не является бинарным, его результаты придется дополнительно нормировать, однако мы не сможем избавиться от относительного порядка на ответах (ответ \enquote{Нет} будет считаться более важным, чем ответ \enquote{Да}).
Таким образом, в текущем виде данный алгоритм нам не подходит.

\subsection{One-Hot Encoding}

Как уже было сказано, нам важно сохранить отсутствие относительного порядка между ответами респондентов, чтобы не вводить будущую модель в заблуждение.
В таких случаях, обычно, применяют алгоритм {\bf One-Hot Encoding}, который преобразует упорядоченные данные в неупорядоченные посредством удаления каждой целочисленной категории и присваивания ей некоторого бинарного значения за каждое уникальную целочисленную категорию этой переменной \cite{feature-eng}.
То есть, полученные значения категорий 1, 2, 3 из результата работы предыдущего алгоритма, раскроются в следующую матрицу:
\[
    A = \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}.
\]
Такой алгоритм кодирования данных прекрасно подходит для наших нужд.

\subsection{Dummy Variable Encoding}

Можно заметить, что в примере из предыдущего абзаца мы храним лишнюю информацию.
В самом деле, нам совершенно не нужен третий столбец матрицы $A$, поскольку из матрицы
\[
    A^{\prime} = \begin{pmatrix}
        1 & 0 \\
        0 & 1 \\
        0 & 0
    \end{pmatrix}
\]
однозначно восстанавливается принадлежность объекта к какой-то категории.
То есть, если человек не ответил \enquote{Да} и не ответил \enquote{Нет}, то мы сразу же делаем вывод, что он затрудняется ответить.
Помимо этого, в некоторых случаях, кодирование через второй алгоритм может сделать матрицу вырожденной \cite{feature-eng-sel}, что плохо сказывается на эффективности и точности некоторых алгоритмов классического машинного обучения (таких как линейная регрессия).
В итоге, для наших наборов данных мы применили именно этот алгоритм с целью приведения ответов респондентов к булевым $m$-мерным векторам для последующего применения алгоритмов топологического анализа данных.
\subsection{Principal Component Analysis}

Большие и массивные наборы данных стали не редкостью и часто включают в себя измерения на многих переменных. 
Зачастую можно значительно уменьшить количество переменных, при этом сохранив большую часть информации в исходном наборе данных. 
Метод  главных компонент (PCA в дальнейшем), вероятно, является наиболее популярным и широко используемым методом уменьшения размерности для этого. 
Положим, что у нас есть $k$ измерений на векторе $x$ из $p$ случайных переменных и мы хотим уменьшить его размерность с $p$ к $q$, где $q$, обычно, намного меньше чем $p$. 
PCA достигает этого посредством нахождения таких линейных комбинаций $a'_1x, a'_2x, ..., a'_qx$, называемых главными компонентами, которые последовательно имеют максимальную вариацию данных, не связанных с предыдущими $a'_kx$.
Решая эту максимизационную задачу мы находим, что векторы $a_1, a_2, ... a_q$ являются собственными значениями матрицы ковариций данных $S$, которая определена как 
\[
    S = \frac{1}{p} \sum_{n = 1}^p(x_n - \overline{x})(x_n - \overline{x})^T
\]
(где $\overline{x} = \frac{1}{p}\sum_{n = 1}^p x_n$, т.е среднее арифметическое).
Собственные значения дают представляют собой дисперии главных компонент, а отношение суммы первых $q$ собственных значений к сумме дисперсий всех $p$ изначальных переменных является долей общей дисперсии в исходном наборе данных, приходящиеся на $q$ главных компонент.

\newpage
\subsubsection*{Пример}
Приведем пример работы PCA на некотором наборе данных (таблица \ref{tab:pca-example}).

\begin{table}[h]
    \begin{center}
        \begin{tabular}{ |c|c|c| } 
            \hline
            Переменная & $a_1$ & $a_2$ \\ 
            \hline
            $x_1$ & $0.34$ & $0.39$ \\ 
            \hline
            $x_2$ & $0.34$ & $0.37$ \\ 
            \hline
            $x_3$ & $0.35$ & $0.10$ \\ 
            \hline
            $x_4$ & $0.30$ & $0.24$ \\ 
            \hline
            $x_5$ & $0.34$ & $0.32$ \\ 
            \hline
            $x_6$ & $0.27$ & $-0.24$ \\ 
            \hline
            $x_7$ & $0.32$ & $-0.27$ \\ 
            \hline
            $x_8$ & $0.30$ & $-0.51$ \\ 
            \hline
            $x_9$ & $0.23$ & $-0.22$ \\ 
            \hline
            $x_{10}$ & $0.36$ & $-0.33$ \\ 
            \hline
        \end{tabular}
    \end{center}
    \caption{Тестовый набор данных из \cite{pca}.}
    \label{tab:pca-example}
\end{table}

Данные состоят из оценок между $0$ и $20$ для $150$ детей возраста $4 \frac{1}{2}$ -- $6$ лет с острова Уайт по 10 предметам. 
Пять тестов были вербальными и пять перфомативными.
Наша таблица показывает векторы $a_1$ и $a_2$, которые являются двумя главными компонентами для этих данных. 
Первая компонента является линейной комбинацией первых десяти оценок с примерно равным весом ($0.36$ --- максимум, $0.23$ минимум) для каждой оценки. 
Сама по себе, эта компонента оценивает около $48\%$ оригинальной изменчивости. 
Вторая компонента сравнивает пять вербальных тестов и пять перфомативных. 
Это учитывает ещё $11\%$ изменчивости. 
Эта форма говорит нам, что после того как мы учли общие способности детей, следующий наиболее важный для нас (линейный) источник изменчивости это разница между детьми, которые хорошо себя показывают в вербальных тестах, относительно успеваемости детей, показатели которых имеют обратный паттерн.

\newpage
\subsection{Uniform Approximation and Projection}

Большинство алгоритмов снижения размерности относятся к одной из двух категорий: факторизация матриц (например, PCA) или отображение графа (например, tSNE).  UMAP представляет собой алгоритм, очень похожий на tSNE, но с рядом ключевых теоретических основ, которые делают его гораздо быстрее. UMAP состоит из двух этапов: построение графика в большей размерности, за которым следует этап оптимизации, чтобы найти наиболее похожий графа в меньшей размерности. Интуиция, лежащая в основе, является достаточно простой: UMAP, по сути, строит взвешенный граф из данных большей размерности, с гранью, показывающей, насколько «близка» данная точка к другой, а затем проецирует этот граф на уменьшенную размерность. Для построения исходного высокоразмерного графа, UMAP полагается на комплекс Чеха, способ представить топологию посредством комбинаторики (вместо геометрии). Для этого мы будем использовать симплициальные комплексы. Используя наши данные как множество симплексов, мы можем зафиксировать представление топологии, что бы получить комплекс Чеха. Мы рассмотрим каждую точку в данных как непрерывную многомерную форму (нашей топологии). Каждая точка является 0-симплексом, таким образом мы можем получать из них некий радиус и соединять точки, которые пересекаются, -- таким образом мы строим 1-, 2- и n-мерные симплексы. Таким образом у нас получается симплициальный комплекс, который достаточно хорошо аппроксимирует топологию датасета, что является прямым следствием теоремы о нерве. Оказывается, что основную часть по представлению топологии выполняют 0- и 1- симплексы и образуют Комплекс Вьеториса - Рипса, с которым гораздо проще работать в вычислительном плане. Используя 0- и 1- симплексы мы можем проецировать граф на аналог меньшей размерности.
К сожалению, данные большого размера представляют собой проблему, которую UMAP необходимо преодолеть - выбрать радиус корректного размера, однако если выбрать слишком маленький радиус и мы будем иметь изолированные, локальные скопления точек, а если взять слишком большой, то все становится связаннымм. Эта проблема усугубляется "curse of dimensionality", когда расстояние между точками становится схожим при большей размерности. Однако UMAP справляется с этой проблемой -- вместо использования фиксированного радиуса, испольузется переменный, определяемый для каждой точки на основе расстояния до k-го ближайшего соседа. Таким образом, соединения становятся вероятными, а дальнейшие точки будут связаны с меньшей вероятностью. Так же добавляется правило, что все точки должны быть соединены с ближайшей соседней точкой. Таким образом, UMAP проецирует данные в более низкую размерность, не теряя скорости (и даже обгоняя tSNE) и точности (используются не только линейные функции, в отличии от PCA), так же отсутствуют ограничения на размерность.

\subsubsection*{Пример}
Самый простой пример использования это преобразование цвета (RGB) в двумерное пространство, если применить алгоритм, то мы мало того, что получим уменьшение размерности, но и столкнёмся с тем, что расположение точек имеет поразительный паттерн, когда видны переходы оттенков цветов.
