\section{Теоретическая часть} \label{sec:theory}
Для начала нам нужно было очистить данные от статистических выбросов -- таким образом, мы избавились от неоднозначных ответов, на подобии тематики секции, названий онлайн-сервисов, которые использовали ученики. 
Так же мы убрали из данных противоречивые ответы на вопросы вопросы, ответы, в которых почти на все вопросы был дан ответ с одинаковой нумерацией (например, школьник на все вопросы отвечал \enquote{Да} или \enquote{1}). 
Над оставшимся массивом данных уже можно было проводить преобразования для отображения ответов каждого респондента в пространство $\{0, 1\}^{m}$. 
Для этого мы воспользовались тремя разными алгоритмами: {\bf Ordinal Encoding}, {\bf One-Hot Encoding} и {\bf Dummy Variable Encoding}.
\subsection{Ordinal Encoding}

В случае алгоритма {\bf Ordinal Encoding}, каждой уникальной категории присваивается уникальное целое число.
Например, категории \enquote{Да}, \enquote{Нет} и \enquote{Затрудняюсь ответить} могут быть закодированы через последовательность 1, 2, 3.
Поскольку такое кодирование является весьма естественным и легко обратимым, мы попробовали применить этот алгоритм для кодирования наших данных, однако быстро отказались от этой идеи.
Поскольку такое кодирование не является бинарным, его результаты придется дополнительно нормировать, однако мы не сможем избавиться от относительного порядка на ответах (ответ \enquote{Нет} будет считаться более важным, чем ответ \enquote{Да}).
Таким образом, в текущем виде данный алгоритм нам не подходит.

\subsection{One-Hot Encoding}

Как уже было сказано, нам важно сохранить отсутствие относительного порядка между ответами респондентов, чтобы не вводить будущую модель в заблуждение.
В таких случаях, обычно, применяют алгоритм {\bf One-Hot Encoding}, который преобразует упорядоченные данные в неупорядоченные посредством удаления каждой целочисленной категории и присваивания ей некоторого бинарного значения за каждое уникальную целочисленную категорию этой переменной \cite{feature-eng}.
То есть, полученные значения категорий 1, 2, 3 из результата работы предыдущего алгоритма, раскроются в следующую матрицу:
\[
    A = \begin{pmatrix}
        1 & 0 & 0 \\
        0 & 1 & 0 \\
        0 & 0 & 1
    \end{pmatrix}.
\]
Такой алгоритм кодирования данных прекрасно подходит для наших нужд.

\subsection{Dummy Variable Encoding}

Можно заметить, что в примере из предыдущего абзаца мы храним лишнюю информацию.
В самом деле, нам совершенно не нужен третий столбец матрицы $A$, поскольку из матрицы
\[
    A^{\prime} = \begin{pmatrix}
        1 & 0 \\
        0 & 1 \\
        0 & 0
    \end{pmatrix}
\]
однозначно восстанавливается принадлежность объекта к какой-то категории.
То есть, если человек не ответил \enquote{Да} и не ответил \enquote{Нет}, то мы сразу же делаем вывод, что он затрудняется ответить.
Помимо этого, в некоторых случаях, кодирование через второй алгоритм может сделать матрицу вырожденной \cite{feature-eng-sel}, что плохо сказывается на эффективности и точности некоторых алгоритмов классического машинного обучения (таких как линейная регрессия).
В итоге, для наших наборов данных мы применили именно этот алгоритм с целью приведения ответов респондентов к булевым $m$-мерным векторам для последующего применения алгоритмов топологического анализа данных.
\subsection{Principal Component Analysis}

Большие и массивные наборы данных стали не редкостью и часто включают в себя измерения на многих переменных. 
Зачастую можно значительно уменьшить количество переменных, при этом сохранив большую часть информации в исходном наборе данных. 
Метод  главных компонент (PCA в дальнейшем), вероятно, является наиболее популярным и широко используемым методом уменьшения размерности для этого. 
Положим, что у нас есть $k$ измерений на векторе $x$ из $p$ случайных переменных и мы хотим уменьшить его размерность с $p$ к $q$, где $q$, обычно, намного меньше чем $p$. 
PCA достигает этого посредством нахождения таких линейных комбинаций $a'_1x, a'_2x, ..., a'_qx$, называемых главными компонентами, которые последовательно имеют максимальную вариацию данных, не связанных с предыдущими $a'_kx$. Решая эту максимизационную задачу мы находим, что векторы $a_1, a_2, ... a_q$ являются собственными значениями матрицы ковариций данных $S$, которая определена как 
    $$S = \frac{1}{p} \sum_{n = 1}^p(x_n - \overline{x})(x_n - \overline{x})^T$$
(где $\overline{x} = \frac{1}{p}\sum_{n = 1}^p x_n$, т.е среднее)
Собственные значения дают представляют собой дисперии главных компонент, а отношение суммы первых $q$ собственных значений к сумме дисперсий всех $p$ изначальных переменных является долей общей дисперсии в исходном наборе данных, приходящиеся на $q$ главных компонент. \\

\subsubsection*{Пример}
Таким образом, мы можем можем привести пример из \cite{pca}

\begin{center}
\begin{tabular}{ |c|c|c| } 
 \hline
 Переменная & $a_1$ & $a_2$ \\ 
 \hline \hline
 $x_1$ & $0.34$ & $0.39$ \\ 
 \hline
 $x_2$ & $0.34$ & $0.37$ \\ 
 \hline
 $x_3$ & $0.35$ & $0.10$ \\ 
 \hline
 $x_4$ & $0.30$ & $0.24$ \\ 
 \hline
 $x_5$ & $0.34$ & $0.32$ \\ 
 \hline
 $x_6$ & $0.27$ & $-0.24$ \\ 
 \hline
 $x_7$ & $0.32$ & $-0.27$ \\ 
 \hline
 $x_8$ & $0.30$ & $-0.51$ \\ 
 \hline
 $x_9$ & $0.23$ & $-0.22$ \\ 
 \hline
 $x_10$ & $0.36$ & $-0.33$ \\ 
 \hline
\end{tabular}
\end{center}

Данные состоят из оценок между $0$ и $20$ для $150$ детей возраста $4 \frac{1}{2}$ -- $6$ лет с острова Уайт по 10 предметам. 
Пять тестов были вербальными и пять перфомативными.
Наша таблица показывает векторы $a_1$ и $a_2$, которые являются двумя главными компонентами для этих данных. 
Первая компонента является линейной комбинацией первых десяти оценок с примерно равным весом ($0.36$ --- максимум, $0.23$ минимум) для каждой оценки. 
Сама по себе, эта компонента оценивает около $48\%$ оригинальной изменчивости. 
Вторая компонента сравнивает пять вербальных тестов и пять перфомативных. 
Это учитывает ещё $11\%$ изменчивости. 
Эта форма говорит нам, что после того как мы учли общие способности детей, следующий наиболее важный для нас (линейный) источник изменчивости это разница между детьми, которые хорошо себя показывают в вербальных тестах, относительно успеваемости детей, показатели которых имеют обратный паттерн.
